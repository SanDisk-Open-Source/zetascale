//----------------------------------------------------------------------------
// ZetaScale
// Copyright (c) 2016, SanDisk Corp. and/or all its affiliates.
//
// This program is free software; you can redistribute it and/or modify it under
// the terms of the GNU Lesser General Public License version 2.1 as published by the Free
// Software Foundation;
//
// This program is distributed in the hope that it will be useful, but WITHOUT
// ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
// FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License v2.1 for more details.
//
// A copy of the GNU Lesser General Public License v2.1 is provided with this package and
// can also be found at: http://opensource.org/licenses/LGPL-2.1
// You should have received a copy of the GNU Lesser General Public License along with
// this program; if not, write to the Free Software Foundation, Inc., 59 Temple
// Place, Suite 330, Boston, MA 02111-1307 USA.
//----------------------------------------------------------------------------

/*
 * File:  fcnl_rms_get_lease_test.c
 * Author: Zhenwei Lu
 *
 * Verify lease renew for get shard meta
 *
 * Created on Febrary 5, 2009, 12:04 AM
 *
 * Copyright Schooner Information Technology, Inc.
 * http://www.schoonerinfotech.com/
 *
 * $Id$
 */

#include "misc/misc.h"

#include "platform/assert.h"
#include "platform/logging.h"
#define PLAT_OPTS_NAME(name) name ## _sn_get_put_test
#include "platform/opts.h"

#include "protocol/replication/copy_replicator_internal.h"

#include "test_common.h"
#include "test_framework.h"

#define NUM_REPLICAS 4
#define EXPIRE_USEC 20
/*
 * We use a sub-category under test because test implies a huge number
 * of log messages out of simulated networking, flash, etc.
 */
PLAT_LOG_SUBCAT_LOCAL(LOG_CAT, PLAT_LOG_CAT_SDF_PROT_REPLICATION,
                      "test/case");

#define PLAT_OPTS_ITEMS_sn_get_put_test() \
    PLAT_OPTS_COMMON_TEST(common_config)

struct common_test_config {
    struct plat_shmem_config shmem_config;
};

#define PLAT_OPTS_COMMON_TEST(config_field) \
    PLAT_OPTS_SHMEM(config_field.shmem_config)

struct plat_opts_config_sn_get_put_test {
    struct common_test_config common_config;
};

/**
 * @brief synchronized create_shard/write/read/delete/delete_shard operations
 */
void
user_operations_get_lease_test(uint64_t args) {
    struct replication_test_framework *test_framework =
        (struct replication_test_framework *)args;
    SDF_shardid_t shard_id = 1;
    struct SDF_shard_meta *shard_meta = NULL;
    struct sdf_replication_shard_meta r_shard_meta;
    /* configuration infomation about shard */
    SDF_replication_props_t *replication_props = NULL;
    SDF_status_t status = SDF_SUCCESS;
    struct cr_shard_meta *in;
    struct cr_shard_meta *out = NULL;
    vnode_t node = 0;
    int failed;
    int i;
    struct timeval expires;
    struct timeval now;

    /* configures test framework accommodate to RT_TYPE_META_STORAGE */
    failed = !(plat_calloc_struct(&replication_props));
    plat_assert(!failed);

    rtfw_set_default_replication_props(&test_framework->config,
                                       replication_props);
    shard_meta = rtfw_init_shard_meta(&test_framework->config,
                                      node /* first_node */,
                                      shard_id
                                      /* shard_id, in real system generated by generate_shard_ids() */,
                                      replication_props);
    plat_assert(shard_meta);

    /*
     * XXX: drew 2009-05-10 Move this to a helper function or get away
     * from the separate SDF_shard_meta type.
     */
    memset(&r_shard_meta, 0, sizeof(r_shard_meta));
    r_shard_meta.type = shard_meta->replication_props.type;
    r_shard_meta.nreplica = shard_meta->replication_props.num_replicas;
    r_shard_meta.current_home_node = node;

    /* XXX: This all will end up on node 0 */
    for (i = 0; i < r_shard_meta.nreplica; ++i) {
        r_shard_meta.pnodes[i] =
            (shard_meta->first_node + i) % test_framework->config.nnode;
    }

    r_shard_meta.meta_pnode  = shard_meta->first_meta_node;

    plat_log_msg(LOG_ID, LOG_CAT, LOG_DBG, "start test_framework");
    rtfw_start(test_framework);
    plat_log_msg(LOG_ID, LOG_CAT, LOG_DBG, "test_framework started\n");

    /*
     * create a shard on test flash to ensure meta data
     * persitence will sucess(currently it fails for no shard).
     * However, that will change replicator test mode
     */
    status = cr_shard_meta_create(&in,
                                  &test_framework->config.replicator_config,
                                  shard_meta);
    plat_assert(status == SDF_SUCCESS);
    plat_assert(in);
    in->persistent.current_home_node = 0;
    in->persistent.lease_usecs = 10000000;

    /* put meta on node0 */
    plat_log_msg(LOG_ID, LOG_CAT, LOG_DBG, "create on node 0");
    status = rtfw_create_shard_meta_sync(test_framework, node, in, &out,
                                         &expires);
    plat_log_msg(LOG_ID, LOG_CAT, LOG_DBG, "create on node 0 complete");
    plat_assert(status == SDF_SUCCESS);
    plat_assert(out);

    out->persistent.lease_usecs = in->persistent.lease_usecs;
    plat_assert(0 == cr_shard_meta_cmp(in, out));
    cr_shard_meta_free(out);

    /*
     * XXX: drew 2009-05-09 sleep and validate that less time remains on the
     * lease.
     */
    /* get meta on node0 */
    plat_log_msg(LOG_ID, LOG_CAT, LOG_DBG, "get on node 0");
    status = rtfw_get_shard_meta_sync(test_framework, node,
                                      in->persistent.sguid, &r_shard_meta, &out,
                                      &expires);
    plat_log_msg(LOG_ID, LOG_CAT, LOG_DBG, "get on node 0 complete");
    plat_log_msg(LOG_ID, LOG_CAT, LOG_TRACE, "get in lease: usec:%d,"
                 "out lease: usec: %d, expire: sec:%d, usec:%d",
                 (int)in->persistent.lease_usecs, (int)out->persistent.lease_usecs,
                 (int)expires.tv_sec, (int)expires.tv_usec);

    plat_assert(status == SDF_SUCCESS);
    plat_assert(out);
    out->persistent.lease_usecs = in->persistent.lease_usecs;
    plat_assert(0 == cr_shard_meta_cmp(in, out));
    cr_shard_meta_free(out);
    
    /* put meta on node0 */
    ++in->persistent.shard_meta_seqno;
    plat_log_msg(LOG_ID, LOG_CAT, LOG_DBG, "put on node 0");
    status = rtfw_put_shard_meta_sync(test_framework, node, in, &out,
                                      &expires);
    plat_log_msg(LOG_ID, LOG_CAT, LOG_DBG, "put on node 0 complete");
    plat_log_msg(LOG_ID, LOG_CAT, LOG_TRACE, "put in lease: usec:%d,"
                 "out lease: usec: %d, expire: sec:%d, usec:%d",
                 (int)in->persistent.lease_usecs, (int)out->persistent.lease_usecs,
                 (int)expires.tv_sec, (int)expires.tv_usec);
    plat_assert(status == SDF_SUCCESS);
    plat_assert(out);

    /* make lease expire */
    expires.tv_sec += 5;
    plat_log_msg(LOG_ID, LOG_CAT, LOG_DBG, "sleep %d secs", (int)expires.tv_sec);
    rtfw_block_until(test_framework, expires);


    /* get meta on node1 */
    plat_log_msg(LOG_ID, LOG_CAT, LOG_DBG, "first get on node 1");
    status = rtfw_get_shard_meta_sync(test_framework, 1,
                                      in->persistent.sguid,
                                      &r_shard_meta, &out, &expires);
    plat_log_msg(LOG_ID, LOG_CAT, LOG_DBG, "first get on node 1 complete");
    plat_log_msg(LOG_ID, LOG_CAT, LOG_TRACE, "get in lease: usec:%d,"
                 "out lease: usec: %d, expire: sec:%d, usec:%d",
                 (int)in->persistent.lease_usecs, (int)out->persistent.lease_usecs,
                 (int)expires.tv_sec, (int)expires.tv_usec);

    plat_assert(status == SDF_SUCCESS);
    plat_assert(out);
    ++in->persistent.shard_meta_seqno;
    ++in->persistent.ltime;
    in->persistent.current_home_node = CR_HOME_NODE_NONE;
    out->persistent.lease_usecs = in->persistent.lease_usecs;
    plat_assert(0 == cr_shard_meta_cmp(in, out));
    cr_shard_meta_free(out);

    rtfw_get_time(test_framework, &now);
    cr_shard_meta_free(in);

    /* Shutdown test framework */
    plat_log_msg(LOG_ID, LOG_CAT, LOG_TRACE,
                 "\n************************************************************\n"
                 "                  Test framework shutdown                       "
                 "\n************************************************************");
    rtfw_shutdown_sync(test_framework);

    /* Terminate scheduler */
    fthKill(1);
}

int main(int argc, char **argv) {
    SDF_status_t status;
    struct replication_test_framework *test_framework = NULL;
    struct replication_test_config *config = NULL;
    int failed;

    struct plat_opts_config_sn_get_put_test opts_config;
    memset(&opts_config, 0, sizeof (opts_config));
    int opts_status = plat_opts_parse_sn_get_put_test(&opts_config, argc, argv);
    if (opts_status) {
        plat_opts_usage_sn_get_put_test();
        return (1);
    }

    failed = !plat_calloc_struct(&config);

    struct plat_opts_config_replication_test_framework_sm *sm_config;
    plat_calloc_struct(&sm_config);
    plat_assert(sm_config);

    /* start shared memory */
    status = framework_sm_init(0, NULL, sm_config);

    /* start fthread library */
    fthInit();

    rt_config_init(config, 10 /* hard code iterations here */);
    config->test_type = RT_TYPE_META_STORAGE;
    config->nnode = 5;
    config->num_replicas = NUM_REPLICAS;

    test_framework = replication_test_framework_alloc(config);
    if (test_framework) {
        plat_log_msg(LOG_ID, LOG_CAT, LOG_TRACE, "test_framework %p allocated\n",
                     test_framework);
    }
    XResume(fthSpawn(&user_operations_get_lease_test, 40960), (uint64_t)test_framework);
    fthSchedulerPthread(0);
    plat_log_msg(LOG_ID, LOG_CAT, LOG_TRACE, "JOIN");
    plat_free(config);
    framework_sm_destroy(sm_config);
    return (0);
}

#include "platform/opts_c.h"
